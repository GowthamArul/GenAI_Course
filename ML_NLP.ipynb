{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0445914-2a47-4e61-b87d-11c80c889be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hey there!\n",
    "Welcome to the Roadmap of the Generative AI Crash Course! \n",
    "Here, you’ll explore NLP’s, ML’s, DL’s, and LLM’s concepts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57e967-7cf0-4860-81e5-3e5feee1a52e",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, such as sentences or words. This is a crucial step in natural language processing.\n",
    "\n",
    "## Sentence to Paragraphs\n",
    "\n",
    "When working with text, you may need to convert sentences into paragraphs for better readability or analysis.\n",
    "\n",
    "### Useful Resources\n",
    "- For more information on tokenization, check the [NLTK Tokenization Documentation](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "- Explore additional resources on natural language processing techniques.\n",
    "\n",
    "### Key Terms\n",
    "- **Tokenization**: The process of breaking text into tokens (words, phrases, symbols).\n",
    "- **Sentence**: A set of words that conveys a complete thought.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abfc7db-52be-4969-9fca-70eb337643a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey there!', 'Welcome to the Roadmap of the Generative AI Crash Course!', 'Here, you’ll explore NLP’s, ML’s, DL’s, and LLM’s concepts.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_doc = sent_tokenize(corpus)\n",
    "print(sent_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e709bc-0935-4d68-b68a-de4724670dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenize:-  ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you', '’', 'll', 'explore', 'NLP', '’', 's', ',', 'ML', '’', 's', ',', 'DL', '’', 's', ',', 'and', 'LLM', '’', 's', 'concepts', '.']\n",
      "Word Punct Tokenize:-  ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you', '’', 'll', 'explore', 'NLP', '’', 's', ',', 'ML', '’', 's', ',', 'DL', '’', 's', ',', 'and', 'LLM', '’', 's', 'concepts', '.']\n",
      "Word Tree Tokenize:-  ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you’ll', 'explore', 'NLP’s', ',', 'ML’s', ',', 'DL’s', ',', 'and', 'LLM’s', 'concepts', '.']\n"
     ]
    }
   ],
   "source": [
    "## Paragrah/Sentence -> Words\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "word_doc = word_tokenize(corpus)\n",
    "word_pun = wordpunct_tokenize(corpus)\n",
    "word_tree = TreebankWordTokenizer().tokenize(corpus)\n",
    "print('Word Tokenize:- ',word_doc)\n",
    "print('Word Punct Tokenize:- ',word_pun)\n",
    "print('Word Tree Tokenize:- ',word_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40508d-94d7-4026-8d59-07fbe642cf06",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefiex or to the roots of words known as a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4f2ee4-2ba8-4318-b301-17e7d47e246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ['eating', 'eats', 'eaten', 'writing', 'write', 'history', 'finally', 'final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22250e3d-eaba-45ba-ae16-02517b84d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'eat', 'eaten', 'write', 'write', 'histori', 'final', 'final']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "stem_tokens = [stemming.stem(i) for i in token]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced256fa-e4cb-4781-9cb1-b3e3da95302a",
   "metadata": {},
   "source": [
    "## RegexpStemmer Class\n",
    "NLTK has RegexpStemmer Class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and remove any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a96f44b-6f6e-4fd5-929a-d83f5dfe0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'eat', 'eaten', 'writ', 'writ', 'history', 'finally', 'final']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stem = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "reg_token = [reg_stem.stem(i) for i in token]\n",
    "print(reg_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fea228-1425-4e3c-a535-458ffa0040d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
