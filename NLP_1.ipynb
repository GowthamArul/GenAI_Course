{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0445914-2a47-4e61-b87d-11c80c889be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hey there!\n",
    "Welcome to the Roadmap of the Generative AI Crash Course! \n",
    "Here, you’ll explore NLP’s, ML’s, DL’s, and LLM’s concepts.\n",
    "\"\"\"\n",
    "token = ['eating', 'eats', 'eaten', 'writing', 'write', 'history', 'finally', 'final']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57e967-7cf0-4860-81e5-3e5feee1a52e",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, such as sentences or words. This is a crucial step in natural language processing.\n",
    "\n",
    "## Sentence to Paragraphs\n",
    "\n",
    "When working with text, you may need to convert sentences into paragraphs for better readability or analysis.\n",
    "\n",
    "### Useful Resources\n",
    "- For more information on tokenization, check the [NLTK Tokenization Documentation](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "- Explore additional resources on natural language processing techniques.\n",
    "\n",
    "### Key Terms\n",
    "- **Tokenization**: The process of breaking text into tokens (words, phrases, symbols).\n",
    "- **Sentence**: A set of words that conveys a complete thought.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e709bc-0935-4d68-b68a-de4724670dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe Corpus is:-\u001b[0m \n",
      " Hey there!\n",
      "Welcome to the Roadmap of the Generative AI Crash Course! \n",
      "Here, you’ll explore NLP’s, ML’s, DL’s, and LLM’s concepts.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence Toeknize:-**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None ['Hey there!', 'Welcome to the Roadmap of the Generative AI Crash Course!', 'Here, you’ll explore NLP’s, ML’s, DL’s, and LLM’s concepts.']\n",
      "Word Tokenize:- \n",
      " ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you', '’', 'll', 'explore', 'NLP', '’', 's', ',', 'ML', '’', 's', ',', 'DL', '’', 's', ',', 'and', 'LLM', '’', 's', 'concepts', '.']\n",
      "Word Punct Tokenize:- \n",
      " ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you', '’', 'll', 'explore', 'NLP', '’', 's', ',', 'ML', '’', 's', ',', 'DL', '’', 's', ',', 'and', 'LLM', '’', 's', 'concepts', '.']\n",
      "Word Tree Tokenize:- \n",
      " ['Hey', 'there', '!', 'Welcome', 'to', 'the', 'Roadmap', 'of', 'the', 'Generative', 'AI', 'Crash', 'Course', '!', 'Here', ',', 'you’ll', 'explore', 'NLP’s', ',', 'ML’s', ',', 'DL’s', ',', 'and', 'LLM’s', 'concepts', '.']\n"
     ]
    }
   ],
   "source": [
    "## Paragrah/Sentence -> Words\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer, sent_tokenize\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(\"\\033[1mThe Corpus is:-\\033[0m \\n\", corpus)\n",
    "# Sentence Tokenizer\n",
    "sent_doc = sent_tokenize(corpus)\n",
    "print(display(Markdown('**Sentence Toeknize:-**')),sent_doc)\n",
    "\n",
    "# Word Tokenizer\n",
    "word_doc = word_tokenize(corpus)\n",
    "print('Word Tokenize:- \\n',word_doc)\n",
    "\n",
    "# Word Tokenizer with punctuation\n",
    "word_pun = wordpunct_tokenize(corpus)\n",
    "print('Word Punct Tokenize:- \\n',word_pun)\n",
    "\n",
    "# Word Tree Tokenizer\n",
    "word_tree = TreebankWordTokenizer().tokenize(corpus)\n",
    "print('Word Tree Tokenize:- \\n',word_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40508d-94d7-4026-8d59-07fbe642cf06",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefiex or to the roots of words known as a lemma.\n",
    "### RegexpStemmer Class\n",
    "NLTK has RegexpStemmer Class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and remove any prefix or suffix that matches the expression.\n",
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22250e3d-eaba-45ba-ae16-02517b84d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:-  ['eating', 'eats', 'eaten', 'writing', 'write', 'history', 'finally', 'final']\n",
      "Stem Token:- \n",
      " ['eat', 'eat', 'eaten', 'write', 'write', 'histori', 'final', 'final']\n",
      "Reg Token:- \n",
      " ['eat', 'eat', 'eaten', 'writ', 'writ', 'history', 'finally', 'final']\n",
      "Snow Token:- \n",
      " ['eat', 'eat', 'eaten', 'write', 'write', 'histori', 'final', 'final']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, RegexpStemmer, SnowballStemmer\n",
    "print(\"Token:- \",token)\n",
    "# Stemming\n",
    "stemming = PorterStemmer()\n",
    "stem_tokens = [stemming.stem(i) for i in token]\n",
    "print(\"Stem Token:- \\n\", stem_tokens)\n",
    "\n",
    "# Regex Stemmer\n",
    "reg_stem = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "reg_token = [reg_stem.stem(i) for i in token]\n",
    "print(\"Reg Token:- \\n\",reg_token)\n",
    "\n",
    "# Snow Stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "snow_token = [snow.stem(i) for i in token]\n",
    "print(\"Snow Token:- \\n\",snow_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df84741-dd6f-483f-aa47-329eed907177",
   "metadata": {},
   "source": [
    "# Wordnet Lemmatizer\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called 'lemma', which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a vaild word that means the same thing.\n",
    "\n",
    "NLTK provide WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89d343d-eba3-49a1-b229-226f62fb5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:- \n",
      " ['eating', 'eats', 'eaten', 'writing', 'write', 'history', 'finally', 'final']\n",
      "Lem Token:- \n",
      " ['eat', 'eat', 'eat', 'write', 'write', 'history', 'finally', 'final']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## POS parameters\n",
    "POS - Noun-n\n",
    "verb - v\n",
    "adjective - a\n",
    "adverb - r\n",
    "\"\"\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "lem_token = [lem.lemmatize(i, pos='v') for i in token]\n",
    "print('Token:- \\n', token)\n",
    "print('Lem Token:- \\n', lem_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a6fa3-e78b-4058-bb3e-c39a54a49ab5",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f27de7eb-e6ca-4971-bb3b-b9c398f5ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "In this paper, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis (SVS) system, \n",
    "which is built from scratch using singing training data mined from music websites. \n",
    "The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, \n",
    "lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, \n",
    "we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in \n",
    "lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual \n",
    "multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, \n",
    "and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: \n",
    "1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, \n",
    "2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost,\n",
    "3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis \n",
    "and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and \n",
    "4) it can synthesize singing voices in multiple languages and multiple singers. \n",
    "We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages \n",
    "(Chinese, Cantonese and English). The results demonstrate that with the singing data purely mined from the Web, \n",
    "DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eaceaef-f6a6-4e61-9199-ff0509060d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "752743b9-af20-40e1-b999-08033ab6a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1c55d-58d7-4e58-aa50-fc54ad72f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StopWords and Filter and then apply Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) #converting list of sentences into words\n",
    "    new_word = [stem.stem(i) for i in words if i not in set(stopwords.words('english'))] # Iterate the words and if the word not in stop words apply stemming\n",
    "    sentences[i] = ' '.join(new_word) # After removing stop wrods and stemming it, join the words into sentences and place it in the same index in sentence list.\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5ebd4d6-e758-4c81-b351-ba3f77e2bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in paper , develop deepsinger , multi-lingual multi-singer sing voice synthesis ( svs ) system , build scratch use sing train data mine music websites .', 'the pipeline deepsinger consist several step , include data crawl , sing accompaniment separation , lyrics-to-singing alignment , data filtration , sing model .', 'specifically , design lyrics-to-singing alignment model automatically extract duration phoneme lyric start coarse-grained sentence level fine-grained phoneme level , design multi-lingual multi-singer sing model base feed-forward transformer directly generate linear-spectrograms lyric , synthesize voice use griffin-lim .', 'deepsinger several advantage previous svs systems : 1 ) best knowledge , first svs system directly mine train data music websites , 2 ) lyrics-to-singing alignment model avoid human efforts alignment label greatly reduce label cost , 3 ) sing model base feed-forward transformer simple efficient , remove complicate acoustic feature model parametric synthesis leverage reference encoder capture timbre singer noisy sing data , 4 ) synthesize sing voice multiple languages multiple singers .', 'we evaluate deepsinger mine sing dataset consist 92 hours data 89 singers three languages ( chinese , cantonese english ) .', 'the result demonstrate sing data purely mine web , deepsinger synthesize high-quality sing voice term pitch accuracy voice naturalness']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "w_stem = WordNetLemmatizer()\n",
    "\n",
    "# Apply StopWords and Filter and then apply Lemmatize\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) #converting list of sentences into words\n",
    "    new_word = [w_stem.lemmatize(i.lower(), pos='v') for i in words if i not in set(stopwords.words('english'))] # Iterate the words and if the word not in stop words apply stemming\n",
    "    sentences[i] = ' '.join(new_word) # After removing stop wrods and stemming it, join the words into sentences and place it in the same index in sentence list.\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd1e54-d6ba-40f8-8e65-1777e302aa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
